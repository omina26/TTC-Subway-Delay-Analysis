---
title: "ML_analysis"
output: html_document
---

```{r imports, message=FALSE, include=FALSE}
library(knitr)
library(tidyverse)
library(caret)
library(Metrics)
library(mgcv)
library(caret)
library(ranger)
library(gbm)
library(doParallel)
library(dplyr)
library(xgboost)
library(broom)
library(kableExtra)
library(patchwork)
```

First, load data.

```{r load data, include=FALSE}
# load data
df = read.csv("../data/merged_data.csv", header = TRUE)
```


From the data frame I will use the following columns in my models: min_delay, bound, line, station, apparent_temperature, relative_humidity, precipitation, snowfall, snow_depth, cloud_cover, wind_speed, wind_gusts, hour, and service_population.

```{r subset to relevant columns}
# subset to relevant columns
selected_vars = c(
  "min_delay", "bound", "line", "station", "apparent_temperature", 
  "relative_humidity", "precipitation", "snowfall", "snow_depth", 
  "cloud_cover", "wind_speed", "wind_gusts", "hour", "service_population"
)

df = df |> 
  select(all_of(selected_vars)) |>
  mutate(
    bound = as.factor(bound),
    line = as.factor(line),
    station = as.factor(station)
  )
```

Now create train/test split.

```{r train test split, include=FALSE}
# set seed
set.seed(754)

# train-test split (70/30)
split_index = createDataPartition(df$min_delay, p = 0.7, list = FALSE)

train_x = df[split_index, !(names(df) %in% "min_delay")]
train_y = df[split_index, "min_delay", drop = FALSE]

test_x  = df[-split_index, !(names(df) %in% "min_delay")]
test_y  = df[-split_index, "min_delay", drop = FALSE]
```


Models that will be compared:
- GLM with Gamma family
- GAM
- Random Forest
- Gradient Boosting
- XGBoost


GLM with Gamma family:

```{r GLM, include=FALSE}
# select predictors excluding 'station'
predictors_glm = c(
  "bound", "line", "apparent_temperature", "relative_humidity",
  "precipitation", "snowfall", "snow_depth", "cloud_cover",
  "wind_speed", "wind_gusts", "hour", "service_population"
)

# prepare training data
train_glm = train_x |> select(all_of(predictors_glm))
test_glm  = test_x |> select(all_of(predictors_glm))

# fit GLM
glm_gamma = glm(
  min_delay ~ .,
  data = cbind(train_y, train_glm),
  family = Gamma(link = "log")
)

# predict
pred_train_glm = predict(glm_gamma, newdata = train_glm, type = "response")
pred_test_glm  = predict(glm_gamma, newdata = test_glm, type = "response")

# evaluate performance
glm_perf = tibble(
  Model = "GLM (Gamma)",
  RMSE_Train = rmse(train_y$min_delay, pred_train_glm),
  MAE_Train = mae(train_y$min_delay, pred_train_glm),
  R2_Train = 1 - sum((train_y$min_delay - pred_train_glm)^2) / sum((train_y$min_delay - mean(train_y$min_delay))^2),
  RMSE_Test = rmse(test_y$min_delay, pred_test_glm),
  MAE_Test = mae(test_y$min_delay, pred_test_glm),
  R2_Test = 1 - sum((test_y$min_delay - pred_test_glm)^2) / sum((test_y$min_delay - mean(test_y$min_delay))^2)
)
```


GAM:

```{r GAM, include=FALSE}
# fit GAM with smooth terms for continuous variables
gam_model = gam(
  min_delay ~ bound + line +
    s(apparent_temperature) +
    s(relative_humidity) +
    s(precipitation) +
    s(snowfall) +
    s(snow_depth) +
    s(cloud_cover) +
    s(wind_speed) +
    s(wind_gusts) +
    s(hour) +
    s(service_population),
  family = Gamma(link = "log"),
  data = cbind(train_y, train_glm)
)

# predict on train and test sets
pred_train_gam = predict(gam_model, newdata = train_glm, type = "response")
pred_test_gam  = predict(gam_model, newdata = test_glm, type = "response")

# evaluate performance
gam_perf = tibble(
  Model = "GAM",
  RMSE_Train = rmse(train_y$min_delay, pred_train_gam),
  MAE_Train = mae(train_y$min_delay, pred_train_gam),
  R2_Train = 1 - sum((train_y$min_delay - pred_train_gam)^2) / sum((train_y$min_delay - mean(train_y$min_delay))^2),
  RMSE_Test = rmse(test_y$min_delay, pred_test_gam),
  MAE_Test = mae(test_y$min_delay, pred_test_gam),
  R2_Test = 1 - sum((test_y$min_delay - pred_test_gam)^2) / sum((test_y$min_delay - mean(test_y$min_delay))^2)
)
```


Random Forest: (used ranger instead of randomForest package since station has a cardinality of 70)

```{r RF, include=FALSE}
# combine training data for formula interface
train_rf_full = cbind(train_y, train_x)
test_rf_full  = cbind(test_y, test_x)

# define training control
ctrl = trainControl(method = "cv", number = 5)

# set up tuning grid
rf_grid = expand.grid(
  mtry = c(3, 5, 7),
  splitrule = "variance",
  min.node.size = c(5, 10)
)

# train with tuning
set.seed(754)
rf = train(
  x = train_x,
  y = train_y$min_delay,
  method = "ranger",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = rf_grid,
  importance = "impurity",
  num.trees = 500
)

# predict and evaluate
pred_train_rf = predict(rf, newdata = train_x)
pred_test_rf  = predict(rf, newdata = test_x)

# evaluate performance
rf_perf = tibble(
  Model = "Random Forest (Tuned)",
  RMSE_Train = rmse(train_y$min_delay, pred_train_rf),
  MAE_Train = mae(train_y$min_delay, pred_train_rf),
  R2_Train = 1 - sum((train_y$min_delay - pred_train_rf)^2) / sum((train_y$min_delay - mean(train_y$min_delay))^2),
  RMSE_Test = rmse(test_y$min_delay, pred_test_rf),
  MAE_Test = mae(test_y$min_delay, pred_test_rf),
  R2_Test = 1 - sum((test_y$min_delay - pred_test_rf)^2) / sum((test_y$min_delay - mean(test_y$min_delay))^2)
)
```


Gradient Boosting:

```{r GB, include=FALSE}
# combine training and test sets for gbm
train_gbm = cbind(train_y, train_x)
test_gbm  = cbind(test_y, test_x)

# register parallel backend
cl = makePSOCKcluster(4)
registerDoParallel(cl)

# tuning grid
gbm_grid = expand.grid(
  n.trees = c(200, 500),
  interaction.depth = c(4, 6, 8),
  shrinkage = c(0.01, 0.05),
  n.minobsinnode = c(5, 10)
)

# train GBM model with tuning
set.seed(754)
gbm = train(
  min_delay ~ ., 
  data = train_gbm,
  method = "gbm",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = gbm_grid,
  verbose = FALSE
)

# Stop the parallel cluster
stopCluster(cl)
registerDoSEQ()

# predict
pred_train_gbm = predict(gbm, newdata = train_gbm, n.trees = gbm$n.trees)
pred_test_gbm  = predict(gbm, newdata = test_gbm,  n.trees = gbm$n.trees)

# best model
kable(gbm$bestTune, caption = "Best Hyperparameters for GBM", digits = 3)

# evaluate performance
gbm_perf = tibble(
  Model = "Gradient Boosting (Tuned)",
  RMSE_Train = rmse(train_y$min_delay, pred_train_gbm),
  MAE_Train = mae(train_y$min_delay, pred_train_gbm),
  R2_Train = 1 - sum((train_y$min_delay - pred_train_gbm)^2) / sum((train_y$min_delay - mean(train_y$min_delay))^2),
  RMSE_Test = rmse(test_y$min_delay, pred_test_gbm),
  MAE_Test = mae(test_y$min_delay, pred_test_gbm),
  R2_Test = 1 - sum((test_y$min_delay - pred_test_gbm)^2) / sum((test_y$min_delay - mean(test_y$min_delay))^2)
)

```


XGBoost:

```{r XGB, include=FALSE}
# one-hot encode predictors for XGBoost
train_matrix = model.matrix(min_delay ~ . - 1, data = cbind(train_y, train_x))
test_matrix  = model.matrix(min_delay ~ . - 1, data = cbind(test_y, test_x))

# extract response variable
train_label = train_y$min_delay
test_label  = test_y$min_delay

# convert to data.frames for caret
train_xgb = as.data.frame(train_matrix)
train_xgb$min_delay = train_y$min_delay
test_xgb = as.data.frame(test_matrix)

# parallel backend
cl = makePSOCKcluster(4)
registerDoParallel(cl)

# tuning grid
xgb_grid = expand.grid(
  nrounds = c(200, 500),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = c(0, 1),
  colsample_bytree = c(0.7, 0.9),
  min_child_weight = c(5, 10),
  subsample = c(0.7, 0.9)
)

# train tuned model
set.seed(754)
xgb = train(
  min_delay ~ .,
  data = train_xgb,
  method = "xgbTree",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = xgb_grid,
  verbose = FALSE
)

# stop parallel backend
stopCluster(cl)
registerDoSEQ()

# predict with tuned model
pred_train_xgb = predict(xgb, newdata = train_xgb)
pred_test_xgb  = predict(xgb, newdata = test_xgb)

# evaluate performance
xgb_perf= tibble(
  Model = "XGBoost (Tuned)",
  RMSE_Train = rmse(train_label, pred_train_xgb),
  MAE_Train = mae(train_label, pred_train_xgb),
  R2_Train = 1 - sum((train_label - pred_train_xgb)^2) / sum((train_label - mean(train_label))^2),
  RMSE_Test = rmse(test_label, pred_test_xgb),
  MAE_Test = mae(test_label, pred_test_xgb),
  R2_Test = 1 - sum((test_label - pred_test_xgb)^2) / sum((test_label - mean(test_label))^2)
)
```


Comparing Models:

```{r model comparison, echo=FALSE}
# combine all performance data frames
model_perf = bind_rows(glm_perf, gam_perf, rf_perf, gbm_perf, xgb_perf)

# Identify row with lowest test RMSE
best_row = which.min(model_perf$RMSE_Test)

# display comparison table
library(knitr)
perf_table = kable(model_perf, digits = 3, caption = "Table 1: Model Performance Comparison") |>
  kable_styling(full_width = FALSE) |>
  row_spec(best_row, bold = TRUE)
  
# save
save_kable(perf_table, "../plots/model_perf_table.html")

perf_table
```

Variable Importance:

```{r GLM summary, echo=FALSE}
# tidy summary with p-values
glm_table = tidy(glm_gamma) |>
  filter(term != "(Intercept)") |>
  mutate(Significant = ifelse(p.value < 0.05, "Yes", "No")) |>
  rename(
    Term = term,
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `p-value` = p.value
  )

# display nicely
glm_imp = kable(glm_table, digits = 3, caption = "Table 2: GLM Coefficients Summary") |>
  kable_styling(full_width = FALSE) |>
  row_spec(which(glm_table$Significant == "Yes"), bold = TRUE)

save_kable(glm_imp, "../plots/glm_coef_table.html")

glm_imp
```

```{r GAM summary, echo=FALSE}
# Parametric terms
param_terms = tidy(gam_model, parametric = TRUE) |>
  filter(term != "(Intercept)") |>
  mutate(Significant = ifelse(p.value < 0.05, "Yes", "No")) |>
  rename(
    Term = term,
    Estimate = estimate,
    `Std. Error` = std.error,
    `t-value` = statistic,
    `p-value` = p.value
  )

# Smooth terms
smooth_terms = tidy(gam_model, parametric = FALSE) |>
  mutate(Significant = ifelse(p.value < 0.05, "Yes", "No")) |>
  rename(
    Term = term,
    `Effective DF` = edf,
    `Reference DF` = ref.df,
    `F-value` = statistic,
    `p-value` = p.value
  )

# Create two kables
gam_imp_par = kable(param_terms, digits = 3, caption = "Table 3: GAM Parametric Coefficients Summary", format = "html") |>
  kable_styling(full_width = FALSE) |>
  row_spec(which(param_terms$Significant == "Yes"), bold = TRUE)

gam_imp_smo = kable(smooth_terms, digits = 3, caption = "Table 4: GAM Smooth Coefficients Summary", format = "html") |>
  kable_styling(full_width = FALSE) |>
  row_spec(which(smooth_terms$Significant == "Yes"), bold = TRUE)

save_kable(gam_imp_par, "../plots/gam_coef_par_table.html")
save_kable(gam_imp_smo, "../plots/gam_coef_smo_table.html")

gam_imp_par
gam_imp_smo
```

```{r vi, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# RF variable importance
vi_rf = varImp(rf)$importance |>
  as_tibble(rownames = "Variable") |>
  arrange(desc(Overall)) |>
  head(5)

p_rf = ggplot(vi_rf, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "(a) Variable Importance for Random Forest (Top 5)", x = NULL, y = "Importance")

# GBM variable importance
vi_gbm = varImp(gbm)$importance |>
  as_tibble(rownames = "Variable") |>
  arrange(desc(Overall)) |>
  head(5)

p_gbm = ggplot(vi_gbm, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "(b) Variable Importance for GBM (Top 5)", x = NULL, y = "Importance")

# XGB variable importance
vi_xgb = varImp(xgb)$importance |>
  as_tibble(rownames = "Variable") |>
  arrange(desc(Overall)) |>
  head(5)

p_xgb = ggplot(vi_xgb, aes(x = reorder(Variable, Overall), y = Overall)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "(c) Variable Importances  for XGBoost (Top 5)", x = NULL, y = "Importance")

# Combine plots
full_plot <- (
  p_rf /
  p_gbm / 
  p_xgb            
) +
  plot_annotation(title = "Figure 1: Variable Importance Across Tuned Tree-Based Models")

# save
ggsave("../plots/imp_plot.png", full_plot)

print(full_plot)
```

